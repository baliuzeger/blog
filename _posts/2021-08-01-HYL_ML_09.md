---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 09: Tips for Deep Learning"
---

## Steps Toward a Successful DL Training

![DL recipe](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_09/recipe.png)

On many traditional ML models, e.g. decision tree or SVM, the performance on the training set is automatically 100% after training, so we don't have to exam the model's performance on the training set but just test the model on the testing set to see if it's overgitting.

However, on DL, the training may just fail, so we have to exam the model's performance on the training set to verify that the training is effective. After that, we can then test the model on the testing set to check if it's overfitting. If the model fails on either the training or the testing set, we have to go back and check whether we have to do something different or apply some recipes in the 3 steps of ML.

With such charateristics, we can see that although DL models usually have much more parameters than traditional ML models, frequently overfitting is not the 1st problem we encounter in DL. Prior to overfitting, the training may just fails, and it's even not so easy to produce overfitting.

### Caution for Recognizing Overfitting

A model is overfitting **only** when a model **performs well on the training set** and **performs poorly on testing set**.

### Scensrios and Their Recipes

For failure on training set, i.e. the training is not effective:
 - New activation function.
 - Adatative learning rate.

For failure on testing, i.e. overfitting:
 - Early stoping
 - Regularization
 - Dropout

## New Activation Function

### Vanishing Gradient

When using sigmoid function as the activation function on deep networks, **vanishing gradient** may occur because the sigmoid function compresses $0 \text{ to } \infty$ into $0 \text{ to } 1$. When it occurs, the gradient vanishes from the output end to the input end, the output end learns fast and the input end learn slowly.Consequently, when the output end already converges, the input end is still random.
