---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture: RNN"
---

## RNN

### Structure & Operation

<p align="center">
    <img src="https://baliuzeger.github.io/sjl/assets/images/HYL_ML_21_RNN/RNN.png" alt="RNN" style="width:700px;"/>
</p>

We can also make deep RNNs that have more than 1 layer of recurrent layers. Such simplest RNN is also named *Elman network*. There's also *Jordan network*, which feed the output back into hidden layers. Jordan network may have better performance than Elman network because the output values are trained with a target, while a hidden layer is not.

![Elman & Jordan Network](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_21_RNN/Elman-Jordan.png)

### Bi-Directionsl RNN

To let the network be able to consider the whole input sequence when it produce the first output, we let 1 2nd network read the input sequence from the end to the beginning, and let sum of the 2 network's output be the final output.

![Bi-directional RNN](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_21_RNN/bi-directional.png)

## LSTM

Because LSTM is now the standard of RNN, people may mean "LSTM" when they say "RNN". We can call the original simple RNN "Simple RNN" or "vanila RNN".


## References

[Youtube ML Lecture 21-1: Recurrent Neural Network (Part I)](https://www.youtube.com/watch?v=xCGidAeyS4M)

[Youtube ML Lecture 21-2: Recurrent Neural Network (Part II)](https://www.youtube.com/watch?v=rTqmWlnwz_0)

[Course website](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html)
