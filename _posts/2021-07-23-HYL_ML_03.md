---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 3, gradient descent."
---

## Gradient Descent Formulation (Primitive)

For a loss function $L(\vec{w})$:
 1. (Randomly) pick initial $\vec{w}^0$.
 2. $\vec{w}^1 = \vec{w}^0 - \eta \nabla L \vert_{\vec{w}=\vec{w}^0}$, where $\eta$ is the learning rate.
 3. repeat step 2 to get $\vec{w}^2$ ... $\vec{w}^N$ after $N$ iterations.

Finally, we can get a local minimum (possibly the global minum).

## Gradient Descent Tip 1: Tuning Learning Rate

### Loss to # of Updates Plots

**Always** plot loss to # of updates to check the performance of the chosen learning rate.

### Adaptive Learning Rates

#### Reduce Learning Rate by Epochs

At the beginning, we are far from the destination, so we use larger learning rate. After several epochs, we are close to the destination, so we reduce the learning rate. For example: $ \eta^t = 1 / \sqrt{1 + t}$

![loss to updates]((https://baliuzeger.github.io/sjl/assets/images/HYL_ML_02/loss-updates.png))

## Reference
[Youtube Link](https://youtu.be/fegAeph9UaA)
[Course website](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html)
