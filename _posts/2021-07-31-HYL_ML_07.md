---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 7: Backpropagation"
---

For applying gradient descent on neural networks, there may be millions or more parameters, and we use backpropagation to compute the gradients efficiently.

![NN](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_07/NN.png)

For an NN with $\theta$ as the vector that represent all the parameters and $L(\theta)$ as the loss function, $L(\theta) = \sum_{n=1}^{N} C^n (\theta)$,

$$\frac{\partial L(\theta)}{\partial w_i} = \sum_{n=1}^{N} \frac{\partial c^n (\theta)}{\partial w_i}$$

then for any example $n$ and parameter $i$,

$$\frac{\partial C^n}{\partial w_i} = \frac{\partial z_p}{\partial w_i} \frac{\partial C^n}{\partial z} $$

where $z$ is the total input at the node of $w_i$ and $z_p = x_i w_i + \dots$

![neuron](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_07/neuron.png)

then in **forward pass**, we compute $\frac{\partial z_p}{\partial w_i} = x_i$ for all parameters.

In **backward pass**, we compute

$$\frac{\partial C^n}{\partial z_p} = \sigma'(z_p) \sum_{backward} w_j \frac{\partial C^n}{\partial z_k}$$

where $w_j$ and $z_k$ pass backward to the node of $z_p$.

## References
[Youtube Link](https://youtube.com/playlist?list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49)

[Course website](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html)
