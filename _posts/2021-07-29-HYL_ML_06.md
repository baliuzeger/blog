---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 6: Brief Introduction to Deep Learning"
---

## History of and Remarks on Deep Learning

![Deep Learning History](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_06/DL-history.png)

The usage of the  term "deep learning" actually changes with time. For example, at around 2006, "deep learning" refers to "the neural networks that uses Restricted Boltzmann Machine for initialization", and "perceptron" indicated the ones without using RBM. while later people found that RBM actually has no effective benefits for NN, so it just draw people's attention back to NN.

The multi-layer perceptron actually has no significant differences from the DNN today.

Why in 1986, people think that "more than 3 hidden layers are not helpful"? I guess due to tha lack of data and GPU.

## Fully Connected Feedforward Network

### The 1st step of ML, choosing the function set:

![NN layers](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_06/NN-layers.png)

![NN function](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_06/NN-function.png)

### The 2nd step of ML, goodness of functions:

### The 3rd step of ML, find the best function


The hidden layers are actually feature extractors that replace the feature engineering. The shift from machine learning to deep learning is actually transforming the questions from feature engineering to neural network structure design.


## References
[Youtube Link](https://youtube.com/playlist?list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49)

[Course website](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html)
