---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 4, Classification: Probabilistic Generative Model."
---

## What are Classification Problems

Input features & output the class that the input should belong to.

![classification problem](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_04/classification.png)

## The effectiveness of Solving Classification as Regression

If we use value 1, 2, 3, 4 ... as the outputs of classes, there may be problems. 1st, there may be penalty to the examples that are "too correct". For example:

![too correct](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_04/too-correct.png)

2nd, such approach implicitly assumes that the neighboring classes (e.g. 1 & 2, 5 & 6) are quantitative closer than other classes, which is false by the meaning of "classes".

## Ideal approaches to classification

![ideal classification](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_04/ideal-classification.png)

## Probabilistic Generative Model

Generative: we can use the trained model to generate new examples.

Here we consider a binary classification problem.

### The 1st step of ML, choosing the function set:

given an example $\vec{x}$, if $P(C_1 \vert \vec{x}) > 0.5$, i.e. the probability that this example is from class $C_1$ > 0.5, then output $C_1$, otherwise output $C_2$. By probability, 

$$ P(C_1 \vert \vec{x}) = \frac{P(\vec{x} \vert C_1)P(C_1)}{P(\vec{x} \vert C_1)P(C_1) + P(\vec{x} \vert C_2)P(C_2)}$$

$P(C_i)$ is the probability that an example is from class $C_i$. so

$$P(C_i) = \frac{N_i}{N_1 + N_2}$$

where $N_i$ is the total count of examples of class $C_i$.

For $P(\vec{x} \vert C_i)$, let's assume Gaussian distribution

$$f_{\mu , \Sigma}(x) = \frac{1}{(2 \pi)^{\frac{D}{2}} \vert \Sigma \vert^{\frac{1}{s}}} e^{- \frac{1}{2}(\vec{x} - \vec{\mu})^T \Sigma^{-1} (\vec{x} - \vec{\mu})}$$

where $\mu$ is **mean** and $\Sigma$ is **covariance matrix**.

Besides, the generative model $P(\vec{x}) = P(\vec{x} \vert C_1)P(C_1) + P(\vec{x} \vert C_2)P(C_2) $

### The 2nd step of ML, goodness of functions:

Let's maximize the likelihood $L(\mu_i,\Sigma_i)$, the probability to generate all the training examples of class $C_i$:

$$L(\mu_i,\Sigma_i) = \prod_{x^k in C_i} f_{\mu_i , \Sigma_i}(x^k)$$


