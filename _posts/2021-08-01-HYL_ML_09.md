---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 09: Tips for Deep Learning"
---

### Recipe of Deep Learning

![DL recipe](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_09/recipe.png)

On many traditional ML models, e.g. decision tree or SVM, the performance on the training set is automatically 100% after training, so we don't have to exam the model's performance on the training set but just test the model on the testing set to see if it's overgitting. However, on DL, the training may just fail, so we have to exam the model's performance on the training set to verify that the training is effective. After that, we can then test the model on the testing set to check if it's overfitting. If the model fails on either the training or the testing set, we have to go back and check whether we have to do something different or apply some recipes in the 3 steps of ML.

With such charateristics, we can see that although DL models usually hace much more parameters than traditional ML models, frequently overfitting is not the 1st problem we encounter in DL. Prior to overfitting, the training may just fails, and it's even not so easy to produce overfitting.

#### Caution for Recognizing Overfitting

A model is overfitting **only** when a model **performs well on the training set** and **performs poorly on testing set**.

