---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 1, Regression"
---

## Notations

$\vec{x^n}$: vector if input # n. The superscript indicates the # of object.

$x^n_i$: feature # i of input # n. The subscript indicates the # of feature/component.

$\hat{y}^n$: the real output # n.

$(\vec{x^n},\hat{y^n})$: a data pair.

$f^* = arg \min\limits_{f}L(f)$: find best function f over loss function L (badness).

or $\vec{w}^* = arg \min\limits_{\vec{w}}L(\vec{w})$: find the best vector of parameters, where $\vec{w}$ defines a function $f$.

## Gradient Descent

For a loss function $L(\vec{w})$:
 1. (Randomly) pick initial $\vec{w}^0$.
 2. $\vec{w}^1 = \vec{w}^0 - \eta \nabla L|_{\vec{w}=\vec{w}^0}$, where $\eta$ is the learning rate.
 3. repeat step 2 to get $\vec{w}^2$ ... $\vec{w}^T$.
 4. $\vec{w}^1$
 5. $\vec{w}^1 = \vec{w}^0 - \eta \nabla L|\limits_{\vec{w}=\vec{w}^0}$

Finally, we can get a local minimum (possibly the global minum).

### On evaluating functions

When evaluating functions. what we really care about



