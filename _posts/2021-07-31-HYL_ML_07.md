---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 7: Backpropagation"
---

For applying gradient descent on neural networks, there may be millions or more parameters, and we use backpropagation to compute the gradients efficiently.

![NN](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_07/NN.png)

For an NN with $\theta$ as the vector that represent all the parameters and $L(\theta)$ as the loss function, $L(\theta) = \sum_{n=1}^{N} C^n (\theta)$,

$$\frac{\partial L(\theta)}{\partial w_i} = \sum_{n=1}^{N} \frac{\partial c^n (\theta)}{\partial w_i}$$

then for any example $n$ and parameter $i$,

$$\frac{\partial C}{\partial w_i}$$ = \frac{}{}

## References
[Youtube Link](https://youtube.com/playlist?list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49)

[Course website](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html)
