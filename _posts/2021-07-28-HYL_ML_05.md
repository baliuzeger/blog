---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 5, Classification: Logistic Regression."
---

## Logistic Regression

### The 1st step of ML, choosing the function set:

If $P(C_1 \vert x) \geq 0.5$, Output $C_1$; Otherwise, Output $C_2$. where

$$P(C_1 \vert x) = \sigma(z) \text{, } z = w \cdot x + b \text{, } \sigma(z) = \frac{1}{1 + e^{-z}}$$

### The 2nd step of ML, goodness of functions:

The likelihood to get the training examples is

$$L(w, b) = \prod_{x^k \in C_1} f_{w,b}(x^k) \prod_{x^k \in C_2} (1 - f_{w,b}(x^k))$$

and then maximize the likelihood by

$$w^*, b^* = arg \max\limits_{w, b}L(w, b) = arg \min\limits_{w, b} -lnL(w,b)$$

$$-lnL(w, b) = - \sum_{n}[\hat{y}^n ln f_{w,b}(x^n)]+ (1 - \hat{y}^n) ln(1 - f_{w,b}(x^n)) \\
= \sum_{n} C(f(x^n), \hat{y}^n)$$

where C is **cross entropy**.

![cross entropy](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_05/cross-entropy.png)

### The 3rd step of ML, find the best function

By gradient descent, we calculate $\frac{\partial (-lnL(w,b))}{\partial w_i}$ and $\frac{\partial (-lnL(w,b))}{\partial b}$, then we update $w$ and $b$ by

$$w_i \gets w_i - \eta \sum_{n} - (\hat{y}^n - f_{w,b}(x^n)) x_i^n$$

$$b \gets b - \eta \sum_{n} - (\hat{y}^n - f_{w,b}(x^n))$$

## Logistic Regression v.s. Linear Regression

![logistic regression vs linear regression](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_05/Logistic-vs-linear.jpg)

## Why not use rms error on logistic regression?

In step 3,

$$\frac{\partial (f_{w,b}(x)- \hat{y}^n)^2}{\partial w_i} = 2(f_{w,b}(x)- \hat{y}^n)f_{w,b}(x)(1-f_{w,b}(x))x_i$$

## References
[Youtube Link](https://youtube.com/playlist?list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49)

[Course website](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html)
