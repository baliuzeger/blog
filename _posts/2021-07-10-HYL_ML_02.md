---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 2, Sources of error."
---

## Sources of error
 - Bias
 - Variance

## Estimator

$\hat{f}$ is the theoretically best function. From training data, we find $f^*$ as an estimator of $\hat{f}$.

## Bias & Viriance of Estimator

Assume the mean of variable $x$ is $\mu$ and the variance of variable $x$ is $\sigma^2$. To estimate the mean of $x$, the estimator of mean $\mu$:

$$m = \frac{1}{N}\sum_{n}x^{n} \neq \mu$$

while

$$E[m] = \frac{1}{N} \sum_{n} E[x^n] = \mu$$

$m$ is an *unbiased* estimator of $\mu$. and

$$Var[m] = \frac{\sigma^2}{N}$$

variance of $m$ depends on the number of samples.

Estimator of variance $\sigma^2$:

$$s^2 = \frac{1}{N}\sum_{n}(x^n - m)^2$$

while

$$E[s^2] = \frac{N-1}{N}\sigma^2 \neq \sigma^2$$

$S^2$ is a biased estimator of $\sigma^2$.
