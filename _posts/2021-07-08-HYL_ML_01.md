---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 1, Regression"
---

## Notations

$\vec{x^n}$: vector if input # n. The superscript indicates the # of object.

$x^n_i$: feature # i of input # n. The subscript indicates the # of feature/component.

$\hat{y}^n$: the real output # n.

$(\vec{x^n},\hat{y^n})$: a data pair.

$f^* = arg \min\limits_{f}L(f)$: find best function f over loss function L (badness).

or $\vec{w}^* = arg \min\limits_{\vec{w}}L(\vec{w})$: find the best vector of parameters, where $\vec{w}$ defines a function $f$.

## Gradient Descent

For a loss function $L(\vec{w})$:
 1. (Randomly) pick initial $\vec{w}^0$.
 2. $\vec{w}^1 = \vec{w}^0 - \eta \nabla L \vert\limits_{\vec{w}=\vec{w}^0}$, where $\eta$ is the learning rate.
 2. $\vec{w}^1 = \vec{w}^0 - \eta \nabla L \vert \limits_{\vec{w}=\vec{w}^0}$, where $\eta$ is the learning rate.
 2. $\vec{w}^1 = \vec{w}^0 - \eta \nabla L \vert_{\vec{w}=\vec{w}^0}$, where $\eta$ is the learning rate.
 3. repeat step 2 to get $\vec{w}^2$ ... $\vec{w}^T$.
 4. $\vec{w}^1$

Finally, we can get a local minimum (possibly the global minum).

### On evaluating functions

When evaluating functions. what we really care about is the loss over *testing set* but not the *training set*.

## Overfitting

If we can truely find the best function, a more complicated model / larger function set yields lower error on training data, but not always yields lower error on testing data. Such phenomenon is *overfitting*. Therefore, it's important to select the suitable model.

## Regularization



