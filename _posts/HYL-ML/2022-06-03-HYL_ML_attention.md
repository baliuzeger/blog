---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture: Self-Attention"
---

## Overview

With a vector sequence as input, self-attention can produce a sequence of vectors (or scalars) of the same length. The produced vectors can then be fed into feedforward networks or further self-attention layers. Conceptually, the production of every vector takes into account the whole input sequence, evaluate some parts as more relevant and let the relevant ones contribute more.

![overview](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_attention/overview.png)



## References

[【機器學習2021】自注意力機制 (Self-attention) (上)](https://youtu.be/hYdO9CscNes)

[【機器學習2021】自注意力機制 (Self-attention) (下)](https://youtu.be/gmsMY5kc-zw)

[Course Website](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php)
