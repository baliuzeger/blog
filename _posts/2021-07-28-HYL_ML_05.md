---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 5, Classification: Logistic Regression."
---

## Logistic Regression

### The 1st step of ML, choosing the function set:

If $P(C_1 \vert x) \geq 0.5$, Output $C_1$; Otherwise, Output $C_2$. where

$$P(C_1 \vert x) = \sigma(z) \text{, } z = w \cdot x + b \text{, } \sigma(z) = \frac{1}{1 + e^{-z}}$$

### The 2nd step of ML, goodness of functions:

The likelihood to get the training examples is

$$L(w, b) = \prod_{x^k \in C_1} f_{w,b}(x^k) \prod_{x^k \in C_2} (1 - f_{w,b}(x^k))$$

and then maximize the likelihood by

$$w^*, b^* = arg \max\limits_{w, b}L(w, b) = arg \min\limits_{w, b} -lnL(w,b)$$

$$-lnL(w, b) = - \sum_{n}[\hat{y}^n ln f_{w,b}(x^n)]+ (1 - \hat{y}^n) ln(1 - f_{w,b}(x^n)) \\
= \sum_{n} C(f(x^n), \hat{y}^n)$$

where C is **cross entropy**.

## References
[Youtube Link](https://youtube.com/playlist?list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49)

[Course website](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html)
