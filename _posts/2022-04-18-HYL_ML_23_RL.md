---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture: Reinforcement Learning"
---

## Overview of Reinforcement Learning

### Scenarios

We are training *agents*. The agents have *observations* to the *environment* and then take *actions*. the action change the environments, and then the environments give the agents *rewards*. The agents are then udpated based on the rewards.

![RL scenario](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_23_RL/scenario.png)

In old days, the models are weaker, so people process the observations into *states* before feed them to the model. Now the 2 terms *observation* and *state* are exchangable.

### Comparing Reinforcement Learning & Supervised Learning

A key difference between supervised learning (classification or regression) and reinfrocement learning: in reinforcement learning, the machines' decision at the current step effect the data (i.e. the environment) of the later steps. In supervised learning, the data is independent from the machines' current decision.

When we want machines to learn complex behaviors, a problem of behavior cloning, i.e. supervised learning, is that the examples provided by human may include useless movements. By behavior cloning, the machines mimic all the behaviors and cannot recognize which movements are important and which are useless. In the worst cases, the machines may learn only the useless parts and we get poor results. Reinforcement learning can avoid such problem.

Also, in some cases, even we don't know what are the correct behaviors and cannot provide examples to the machines. Reinforcement learning is suitable for such kind of tasks.

### Difficulty of Reinforcement Learning

The reward may be sparse, e.g. in Go, there's reward only at the end of a game.

### Approaches of Reinforcement Learning

Policy-based: learning an actor. Value-based: learning a critic. A critic evaluates the goodness of states or actions of states.

![RL-approaches](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_23_RL/RL-approaches.png)

### Learning by Demonstration

Also *imitation learning* or *apprenticeship learning*. An expert demonstrate how to solve the task, and the machine learns from the demonstration. Not behavior cloning! Inverse reinforcement learning is an example.

### Comparing Inverse Reinforcement Learning & Behavior Cloning

In supervised learning, we expect training and testing data have the same distribution. But for learning complex behavior, the 2 distribution may vary a lot. IRL can overcome such problem by learning the reward function.

<p align="center">
    <img src="https://baliuzeger.github.io/sjl/assets/images/HYL_ML_23_RL/different-data-distribution.png" alt="different-data-distribution" style="width:400px;"/>
</p>

## Policy-Based

By the policy-based approaches, we are learning *actors* $\pi$, which are functions that $\pi(observation) = action$. In the mean time, we use the reward from the environment to find the best function.

![policy-based-overview](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_23_RL/policy-based-overview.png)

### ML Step 1: Defining a Function Set

For depp reinforcement learning, we use neural networkd as actors. We define the output as the probabilities of the actions. Letting the actor be stochastic may benefit the training because the randomness can let it perform more different trials.

![neural network as an actor](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_23_RL/actor-NN.png)

By the old reinforcement learning, we use lookup tables. However, in many cases, we cannot exhaust all the possibility of the observation. We need neural networks' power of generalization.

### ML Step 2: Defining Goodness of functions

We have an actor $\pi_{\theta}(s)$ where $\theta$ represent the parameters of the NN and $s$ in the input observations. When we use the actor $\pi_{\theta}(s)$ to perform the task, at every step $t$, we have observation $s_t$, the actor produce action $a_t$, and the environment provide reward $r_t$. Letting the actor perform the task 1 time is an *episode*. An episode has a trakectory

$$ \tau = \{ s_1, a_1, r_1, \dotsc, s_T, a_T, r_T \} $$

and a total reward

$$ R(\tau) = \sum_{t=1}^T r_t $$

The expected value of total reward over parameter $\theta$ over all possible trajectories is then

$$ \overline{R_{\theta}} = \sum_{\tau} R(\tau) P(\tau \vert \theta) \approx \frac{1}{N} \sum_{n=1}^{N} R(\tau^n) $$

Where $N$ is the total times we use $\pi_{\theta}$ to perform the task. We then use $\overline{R_{\theta}}$ as the goodness of the actor function.

### ML Step 3: Finding the Best Function

We want to find the $\theta$ that maximize the expected reward

$$ \theta^* = arg \max\limits_{\theta} \overline{R_{\theta}}$$

By gradient **ascent**,

$$ \theta^{new} \gets \theat^{old} + \eta \nabla \overline{R_{\theta^{old}}} $$

after some derivation, we get

![gradient-R](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_23_RL/gradient-R.png)

Furthermore, it's possible that $R(\tau^n)$ is always possitive. In the ideal cases, it's not a problem. After an update, the probability of the action with the less positive update will actually decrease. However, in practical cases, we samples over the actions, and some actions may never be sampled. If all the $R(\tau^n)$s are positive, the unsampled actions will definitely decrease, and the machine may never try such actions and that may be a unwanted results.

![all-plus-problem](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_23_RL/all-plus-problem.png)

To have more effective plus / minus values of the updates, we subtract a baseline $b$:

![gradient-baseline](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_23_RL/gradient-baseline.png)




![](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_23_RL/.png)

<p align="center">
    <img src="https://baliuzeger.github.io/sjl/assets/images/HYL_ML_23_RL/.png" alt="" style="width:700px;"/>
</p>


## Value-Based

## Inverse Reinforcement Learning

Do the machines copy the useless movements by IRL? I think IRL doesn't guarantee avoiding such problem

## References

[Youtube ML Lecture 23-1: Deep Reinforcement Learning](https://youtu.be/W8XF3ME8G2I)

[Youtube ML Lecture 23-2: Policy Gradient (Supplementary Explanation)](https://youtu.be/y8UPGr36ccI)

[Youtube ML Lecture 23-3: Reinforcement Learning (including Q-learning)](https://youtu.be/2-JNBzCq77c)

[Course website](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html)
