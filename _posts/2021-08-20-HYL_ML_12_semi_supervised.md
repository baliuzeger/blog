---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 12: Semi-Supervised Learning"
---

In supervised learning, all the training data are labelled, i.e. we have

$$\{(x^r, \hat{y}^r)\}^{R}_{r=1}$$

While in some scenarios, collecting data is easy but collecting *labelled* data is expensive, so we may have some labelled data and many unlabelled data, i.e. we have

$$\{(x^r, \hat{y}^r)\}^{R}_{r=1}, \{ x^u \}^{R+U}_{u=R+1}$$

Further more, **transductive learning** is the case that unlabeled data is the testing data, i.e. trying to be sepcific on the problem / task, and **inductive learning** unlabeled data is not the testing data, i.e. trying to be be general.

## Semi-Supervised Learning for Generative Models

### Steps

Consider a binary classifgication,

**Step 0**

Initialze the model by the labelled data as in supervised learning:

$$\theta = \{  P(C_1), P(C_2), \mu^1, \mu^2, \Sigma \} $$

**Step 1**

Compute the posterior probability of all the unlabeled data $P_{\theta}(C_1 \vert x^u)$ by the calculated $\theta$.

**Step 2**

Update the model $\theta$ by

$$P(C_1) = \frac{N_1 + \sum_{x^u} P(C_1 \vert x^u)}{N_{tot}}$$

$$\mu^1 = \frac{1}{N_1} \sum_{x^r \in C_1} x^r + \frac{1}{ \sum_{x^u}P(C_1 \vert x^u) } \sum_{x^u}P(C_1 \vert x^u) \dots$$

and then repeat from step 1 until $\theta$ converges. $N_{tot}$ is the total number of examples, $N_1$ is the number of examples belonging to $C1$.

### Explanation

We are finding the maximum likelihood with labelled data and unlabelled data

$$ lnL(\theta) = \sum_{x^r} ln P_{\theta}(x^r, \hat{y}^r) + \sum_{x^u} lnP_{\theta}(x^u) $$

while it's non-convex, so we perform a **EM algorithm** to solve it iteratively, where

$$ P_{\theta}(x^u) = P_{\theta}(x^u \vert C_1)P(C_1) + P_{\theta}(x^u \vert C_2)P(C_2) $$

here the labels we make on the originally unlabelled data are **soft labels**, i.e. an example belong to the classes by probability.


## Low Density Seperation Assumption

非黑即白。"It's either black or white."

We assume that at the boundary of classes, the density of examples are relatively low.

### Self-Training

Using the labelled data to train the model first, and then apply the model to the unlabelled data to get more labelled data for further iterative training.

Self-training is a general methodology, so it can be apply on any kind of models.

(My argument) By the low density assumption, the boundary between classes make by the model trained by the labelled data should at least cross some of the low density region, so the model should at least correctly classify some unlabelled data. By choosing the correctly classified unbelled data with some criteria and add them into to labelled data and repeat such processes, we should be able to gradually enparge the labelled data set and improve the performance of the model.

#### Steps

Given labelled data set

$$\{(x^r, \hat{y}^r)\}^{R}_{r=1}$$

and unlabelled data set

$$ \{ x^u \}^{R+U}_{u=R+1} $$

Repeat the following steps:
1. Train model $f^*$ from labelled data set.
2. Apply $f^*$ to the unlabelled data set to get **pseudo label**.
3. Remove a set of data from unlabeled data set, and add them into the labeled data set.

In step 3, how to choose the data set remains open. You can also provide a weight to each data.

#### Discussions

Self-training is useless on regression because there will be no error by the $(x^u, y^u)$ generated by $f^*$. By the same reason, when using self-training on neural networks, we must use hard label.

### Entropy-Based Regularization

(My argement) By low density separation approximation, most examples should belong to a class with high probability and to other classes with low probality, so the entropy should be low for a successful classifier.

By entropy-based regularization, we minimize the loss function

$$L = \sum_{x^r} C(x^r, \hat{y}^r) + \lambda \sum_{x^u}E(y^u) $$

where $C$ is cross entropy and the entropy of the predictions on the unlabelled data

$$ E(y^u) = - \sum_{m = \text{all classes}}y^u_m ln(y^u_m) $$

### Semi-Supervised SVM

![semi-supervised SVM](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_12/semi-SVM.png)

## Smoothness Assumption

### Cluster and Label

### Graph-Based Approach

## Better Representation

去蕪存菁，化繁為簡。

Find the latent factors behind the observation, where the latent factors (usually simpler) are better representations.
