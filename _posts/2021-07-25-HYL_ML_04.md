---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 4, Classification: Probabilistic Generative Model."
---

## What are Classification Problems

Input features & output the class that the input should belong to.

![classification problem](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_04/classification.png)

## The effectiveness of Solving Classification as Regression

If we use value 1, 2, 3, 4 ... as the outputs of classes, there may be problems. 1st, there may be penalty to the examples that are "too correct". For example:

![too correct](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_04/too-correct.png)

2nd, such approach implicitly assumes that the neighboring classes (e.g. 1 & 2, 5 & 6) are quantitative closer than other classes, which is false by the meaning of "classes".

## Ideal approaches to classification

![ideal classification](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_04/ideal-classification.png)

## Probabilistic Generative Model

Generative: we can use the trained model to generate new examples.

Here we consider a binary classification problem.

The 1st step of ML, choosing the function set:

given an example $\vec{x}$, if $P(C_1 \vert \vec{x}) > 0.5$, i.e. the probability that this example is from class $C_1$ > 0.5, then output $C_1$, otherwise output $C_2$. By probability, 

$$ P(C_1 \vert \vec{x}) = \frac{P(\vec{x} \vert C_1)P(C_1)}{P(\vec{x} \vert C_1)P(C_1) + P(\vec{x} \vert C_2)P(C_2)}$$
