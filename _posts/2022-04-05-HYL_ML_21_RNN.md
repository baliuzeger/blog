---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture: RNN"
---

## RNN

<p align="center">
    <img src="https://baliuzeger.github.io/sjl/assets/images/HYL_ML_21_RNN/RNN.png" alt="RNN" style="width:700px;"/>
</p>

We can also make deep RNNs that have more than 1 layer of recurrent layers. Such simplest RNN is also named *Elman network*. There's also *Jordan network*, which feed the output back into hidden layers. Jordan network may have better performance than Elman network because the output values are trained with a target, while a hidden layer is not.

<p align="center">
    <img src="https://baliuzeger.github.io/sjl/assets/images/HYL_ML_21_RNN/Elman-Jordan.png" alt="Elman & Jordan Network" style="width:700px;"/>
</p>



## LSTM

Because LSTM is now the standard of RNN, people may mean "LSTM" when they say "RNN". We can call the original simple RNN "Simple RNN" or "vanila RNN".


## References

[Youtube ML Lecture 21-1: Recurrent Neural Network (Part I)](https://www.youtube.com/watch?v=xCGidAeyS4M)

[Youtube ML Lecture 21-2: Recurrent Neural Network (Part II)](https://www.youtube.com/watch?v=rTqmWlnwz_0)

[Course website](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html)
