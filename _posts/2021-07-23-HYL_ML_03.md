---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 3, gradient descent."
---

## Vanilla Gradient Descent Formulation

For a loss function $L(\vec{w})$:
 1. (Randomly) pick initial $\vec{w}^0$.
 2. $\vec{w}^1 \gets \vec{w}^0 - \eta \nabla L \vert_{\vec{w}=\vec{w}^0}$, where $\eta$ is the learning rate.
 3. repeat step 2 to get $\vec{w}^2$ ... $\vec{w}^T$ after $T$ iterations.

Finally, we can get a local minimum (possibly the global minum).

## Gradient Descent Tip 1: Tuning Learning Rate

### Loss to # of Updates Plots

**Always** plot loss to # of updates to check the performance of the chosen learning rate.

![loss to updates](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_03/loss-updates.png)

### Adaptive Learning Rates

#### Reduce Learning Rate by Epochs

At the beginning, we are far from the destination, so we use larger learning rate. After several epochs, we are close to the destination, so we reduce the learning rate. For example: $ \eta^t = 1 / \sqrt{1 + t}$.

#### Use Different Rate for Different Parameters

Learning rate cannot be one-size-fits-all, so give different parameters different learning rates.

#### Adagrad

Divide the learning rate of each parameter by the **root mean square of its previous derivatives**.

$$ w^{t+1}_{j} \gets w^{t+1}_{j} - \frac{\eta^t}{\sigma^t_j} g^t_j$$

where $ w^{t}_{j} $ is the $j$th parameter at the $t$th iteration, 

$ \eta^t = 1 / \sqrt{1 + t} $, 
$ g^t = \frac{\partial L(\vec{w}^t)}{\partial w^t_j} $ and 
$ \sigma^t_j = \sqrt{\frac{1}{t+1} \sum_{k=0}^{t}\vert g^k \vert^2} $. then

$$ w^{t+1}_{j} \gets w^{t+1}_{j} - \frac{\eta}{\sqrt{\sum_{k=0}^{t} \vert g^k \vert^2}} g^t_j$$



## Reference
[Youtube Link](https://youtu.be/fegAeph9UaA)

[Course website](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html)
