---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 1, Regression"
---

## Notations

$\vec{x^n}$: vector if input # n. The superscript indicates the # of object.

$x^n_i$: feature # i of input # n. The subscript indicates the # of feature/component.

$\hat{y}^n$: the real output # n.

$(\vec{x^n},\hat{y^n})$: a data pair.

$f^* = arg \min\limits_{f}\mathrm{L}(f)$: find best function f over loss function L.

or $\vec{w}^* = arg \min\limits_{\vec{w}}\mathrm{L}(\vec{w})$: find the best vector of parameters, where $\vec{w}$ defines a function $f$.

## Gradient Descent


