---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture: Normalization"
---

## Normalization and Error Surface

Like the left part of the figure, the error surface may be steep for some parameters while flat for some others, and such situation cause sifficulties for gradient descent. By normalization, we made the slope of the error surface to different parameters more uniform ro reduce the difficulty of gradient descent, like the right part of the figure.

![error-surface](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_normalization/error-surface.png)
![](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_normalization/.png)
![](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_normalization/.png)

## Feature Normalization

## Batch Normalization


## References

[Youtube 【機器學習2021】類神經網路訓練不起來怎麼辦 (五)： 批次標準化 (Batch Normalization) 簡介](https://youtu.be/BABPWOkSbLE)

[Course website](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html)
