---
layout: post
title: "Notes for Prof. Hung-Yi Lee's ML Lecture 10: CNN"
---

# CNN: Convolutional Neural Networks

When using fully connected networks on images, the number of parameters may be too large. By applying constraints on the networks based on properties of images, we can reduce the total number of parameters.

The fundamental concept of CNN is "to let filters convolve through the image".

## Input Images

We have to transform the input image to the size of the input of the network in advance.

The color images have 3 channels (RGB) on every pixel; the black-white have only 1 channel.

## Properties of Images & the CNN Technuques

**When using the CNN techniques, make sure that your task meet the corresponding properties**. For example AlphaGo don't use pooling; NLP/Speech recognition only convolve through a single dimension (word sequence / frequency). In NLP, the vectors on the other dimension are not spatially related to the neighboring ones (word vector dimentions ); in speech recognition, the shift-invariant effect on the dimention of time is handled by recurrentness of the network.

### Critical Patterns in Small Areas: Filters

In images, the critical patterns for recognition are usually in areas that are smaller than the whole image or even in small regions, so we use **filters** to find the patterns. The area that a filter processes is the **receptive field**.

![filter](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_10/filter.png)

### Critical Patterns at Different Regions: Weight Sharing

In images, the critical patterns may appear in different regions of the images, so the filters for the same pattern at different locations should **share their weights**.

![weights sharing](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_10/weight-sharing.png)

### Subsampling Don't Impact the Recognition: Pooling

Subsampling of the image don't change the object, so we can apply **pooling** to further reduce the total number of parameters.

![subsampling](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_10/subsampling.png)

Recently some networks don't use pooling because the computing cost is not too high for their computing power.

## CNN Structure

![cnn structure](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_10/cnn-structure.png)

## Total Number of Neurons & Connections for a Convolution Layer

Stride: the pixel distance between the neighboring receptive filds.

Padding: when using padding, if a receptive field at the edge exceeds the edge, then the exceeding part will be filled based on the padding method. Usually we use 0 padding, i.e. pad with 0's.

Feature map: The output map produced by a filter.

![convolution](https://baliuzeger.github.io/sjl/assets/images/HYL_ML_10/convolution.png)

Assume the shape of input and output are squares. Total number of weights of the convolution layer:

$$n_f (n_{rs})^2 n_{ch}$$

where $n_f$ is the total number of filters, $n_{rs}$ is the size of one side of the receptive field, $n_{ch}$ is the total number of channels of the input.

The total number of output neurons:

$$n_{fm} n_f$$

where $n_{fm}$ is the total number of neurons of a feature. with padding to the edge,

$$n_{fm} = (\frac{n_{in}}{n_s} \text{ round up})^2 $$

without padding to the edge,

$$n_{fm} = (\frac{n_{in} - n_{rs} + 1}{n_s} \text{ round up})^2 $$

where $n_s$ is stride.

## Data Augmentation

CNN on rotation & scaling

